{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "This is a more extended example than `ex02.ipynb` to look at the different ways we can do the same thing in TensorFlow. In particular, this shows one of the major themes in TensorFlow - the trade-off between detailed analytical work done by hand and the abstract pre-fabricated solutions available off-the-shelf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "P = 1\n",
    "slope = 5\n",
    "intercept = 2\n",
    "X_train = np.random.normal(size=[N, P])\n",
    "Y_train = X_train * slope + intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Let's define our prediction as:\n",
    "$$\\widehat{y}=x\\beta+b\\left(\\begin{array}{c}1\\\\1\\\\\\vdots\\\\1\\end{array}\\right)=x\\beta+b\\mathbb{1}_N$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define our data for training / evaluation.\n",
    "# A tf.placeholder is a recipe for an input. Here, we say that each has the shape [None, 1]\n",
    "# which means that we are going to provide an arbitrary number of inputs that each are one-dimensional.\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"x\")\n",
    "Y = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"y\")\n",
    "\n",
    "# Define the model. This includes our variables (beta and b) and how they interact with our inputs.\n",
    "# We'll initialize them to 0 (arbitrarily).\n",
    "beta = tf.Variable(tf.zeros((1, 1)))\n",
    "b = tf.Variable(tf.zeros((1, 1)))\n",
    "Y_pred = tf.matmul(x, beta) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and gradient\n",
    "The loss is the sum of the squared errors:\n",
    "\\begin{align}\n",
    "\\mathcal{L} &= (Y - \\hat{y})^\\top (Y-\\hat{y})\n",
    "\\\\\n",
    "&= (Y-(X\\beta+b\\mathbb{1}_N))^\\top(Y-(X\\beta+b\\mathbb{1}_N))\n",
    "\\\\\n",
    "&=Y^\\top Y - 2 Y^\\top (X\\beta+b\\mathbb{1}_N)+(X\\beta+b\\mathbb{1}_N)^\\top(X\\beta+b\\mathbb{1}_N)\n",
    "\\end{align}\n",
    "\n",
    "The gradient of the loss with respect to our variables ($\\beta$ and $b$) is:\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\beta} = - 2 X^\\top Y + 2 X^\\top X \\beta + 2 N b \\bar{x}$$\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial b} = - 2 N \\bar{Y} + 2 N \\bar{x}^\\top\\beta + 2 N b$$\n",
    "\n",
    "Where $\\bar{Y}=\\frac{1}{N}\\sum_i Y_i$ and $\\bar{x}_j=\\frac{1}{N}\\sum_{i=1}^N X_{i,j}$\n",
    "\n",
    "(as all good write-ups, the derivation of the above is left to the reader)\n",
    "\n",
    "[Note: We could use the mean squared error instead and pick up a multiplicative factor of $\\frac{1}{N}$ everywhere (loss function and each component of the gradient), but the minimum will occur at the same place, so it's somewhat moot as long as our batch size stays constant]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow as calculator\n",
    "If we know an answer analytically, we can still use TensorFlow to compute it. Because we're encoding the bias term separately (trying to be more like the CS literature of having separate weights and biases when we get to neural networks, but diverging from the standard column-of-ones treatment we use in statistics), we get a slight update for the standard solution to linear regression (that we can derive by setting the gradient [above] to zero):\n",
    "\n",
    "\\begin{align}\n",
    "\\left(X^\\top X - N \\bar{x}\\bar{x}^\\top\\right)\\hat{\\beta} &= \\left(X^\\top Y - N\\bar{x}\\bar{Y}\\right)\n",
    "\\\\\n",
    "\\hat{b} &= \\bar{y}-\\bar{x}^\\top \\hat{\\beta}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xt = tf.transpose(x)\n",
    "batch_size = tf.cast(tf.shape(x)[0], tf.float32)\n",
    "x_mean = tf.reshape(tf.reduce_mean(xt, 1), (-1, 1))\n",
    "Y_mean = tf.reduce_mean(Y)\n",
    "\n",
    "# Set up the linear equation to be solved, like you might do in R.\n",
    "lhs = tf.matmul(xt, x) - batch_size * tf.matmul(x_mean, tf.transpose(x_mean))\n",
    "rhs = tf.matmul(xt, Y) - batch_size * x_mean * Y_mean\n",
    "# Solved via LU decomposition internally.\n",
    "beta_hat = tf.matrix_solve(lhs, rhs)\n",
    "b_hat = Y_mean - tf.matmul(tf.transpose(x_mean), beta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "And we get a result within $\\epsilon$ of the values used to generate the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 4.99999905]], dtype=float32), array([[ 2.00000024]], dtype=float32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run([beta_hat, b_hat], {x:X_train, Y:Y_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Stochastic) Gradient Descent\n",
    "Matrices can be messy, and if our data set is large (especially if $P$, the number of dimensions in the input, is high), we might only be able to do small batches of work at a time. In particular, we resort to gradient descent (or its minibatch cousin stochastic GD) to solve the problem.\n",
    "\n",
    "One way to do that is to analytically compute the gradients and do a small update each pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xt = tf.transpose(x)\n",
    "x_mean = tf.reshape(tf.reduce_mean(xt, 1), (-1, 1))\n",
    "Y_mean = tf.reduce_mean(Y)\n",
    "batch_size = tf.cast(tf.shape(x)[0], tf.float32)\n",
    "\n",
    "# Gradients\n",
    "dloss_dbeta = - 2 * tf.matmul(xt, Y) + 2 * tf.matmul(tf.matmul(xt, x), beta) + 2 * batch_size * b * x_mean\n",
    "dloss_db = - 2 * batch_size * Y_mean + 2 * batch_size * tf.matmul(tf.transpose(x_mean), beta) + 2 * b * batch_size\n",
    "\n",
    "# Updates\n",
    "learning_rate = 0.002\n",
    "new_beta = beta - learning_rate * dloss_dbeta\n",
    "new_b = b - learning_rate * dloss_db\n",
    "\n",
    "# Group the updates into a single TensorFlow operation. The tf.assign operation will\n",
    "# Take the second value and store it in the first Variable.\n",
    "update_rule = tf.group(tf.assign(beta, new_beta), tf.assign(b, new_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Here, we can run on the full data set (although we could just take slices instead), and show how the parameters quickly converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.0793245, 1.1530027)\n",
      "(3.2712488, 1.6138525)\n",
      "(3.9686174, 1.8903623)\n",
      "(4.3761468, 2.0070336)\n",
      "(4.6174707, 2.0482645)\n",
      "(4.7623405, 2.0457377)\n",
      "(4.8512335, 2.0442216)\n",
      "(4.9057765, 2.0370359)\n",
      "(4.9396844, 2.0288739)\n",
      "(4.9610252, 2.020076)\n",
      "(4.9747148, 2.0147974)\n",
      "(4.9834709, 2.0106635)\n",
      "(4.9891248, 2.0071659)\n",
      "(4.9928336, 2.0050673)\n",
      "(4.9952531, 2.0035465)\n",
      "(4.9968424, 2.0024631)\n",
      "(4.9978919, 2.0017009)\n",
      "(4.9985886, 2.0011694)\n",
      "(4.9990525, 2.0007687)\n",
      "(4.9993649, 2.0005281)\n"
     ]
    }
   ],
   "source": [
    "for _ in xrange(20):\n",
    "    sess.run(update_rule, feed_dict={x: X_train, Y: Y_train})\n",
    "    beta_np, b_np = sess.run([beta, b], feed_dict={x: X_train, Y: Y_train})\n",
    "    print(beta_np[0, 0], b_np[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There must be a better way\n",
    "This may seem like a lot of math for a machine learning system that's supposed to \"just work\". If you thought that, you'd be right.\n",
    "\n",
    "There's a lot we can do to make this whole process easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation\n",
    "One of the ways we could do GD/SGD above a lot easier is if we didn't have to compute the derivative by hand. Well, we're in luck: TensorFlow has the `tf.gradients` function which will do symbolic differentiation with respect to variables in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum((Y - Y_pred) ** 2)\n",
    "dloss_dbeta, dloss_db = tf.gradients(loss, [beta, b])\n",
    "# dloss_db = tf.gradients(loss, b)[0]\n",
    "\n",
    "# Updates\n",
    "learning_rate = 0.002\n",
    "new_beta = beta - learning_rate * dloss_dbeta\n",
    "new_b = b - learning_rate * dloss_db\n",
    "\n",
    "# Group the updates into a single TensorFlow operation. The tf.assign operation will\n",
    "# Take the second value and store it in the first Variable.\n",
    "update_rule = tf.group(tf.assign(beta, new_beta), tf.assign(b, new_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "And here we are, getting the same results for the same learning rate, without having to compute the gradient by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.0793245, 1.1530027)\n",
      "(3.2712493, 1.6980028)\n",
      "(3.962677, 1.9408524)\n",
      "(4.3689451, 2.0377469)\n",
      "(4.6108928, 2.0672009)\n",
      "(4.7569761, 2.0677917)\n",
      "(4.8463922, 2.0578327)\n",
      "(4.9018512, 2.0455444)\n",
      "(4.9366808, 2.034256)\n",
      "(4.9588065, 2.0250239)\n",
      "(4.9730077, 2.0179226)\n",
      "(4.9822054, 2.0126593)\n",
      "(4.9882092, 2.0088518)\n",
      "(4.9921546, 2.0061436)\n",
      "(4.9947619, 2.00424)\n",
      "(4.9964929, 2.0029137)\n",
      "(4.9976468, 2.0019958)\n",
      "(4.9984179, 2.0013635)\n",
      "(4.9989347, 2.0009298)\n",
      "(4.9992819, 2.000633)\n"
     ]
    }
   ],
   "source": [
    "for _ in xrange(20):\n",
    "    sess.run(update_rule, feed_dict={x: X_train, Y: Y_train})\n",
    "    beta_np, b_np = sess.run([beta, b], feed_dict={x: X_train, Y: Y_train})\n",
    "    print(beta_np[0, 0], b_np[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing the explicit update\n",
    "The next annoying thing in our code above is that we've had to implement gradient descent ourselves. It would be nice if we didn't have to do that ourselves.\n",
    "\n",
    "TensorFlow has several built-in optimizers:\n",
    "- Gradient descent\n",
    "- Adam\n",
    "- Adagrad\n",
    "- RMSProp\n",
    "- etc.\n",
    "\n",
    "We can use them to compute gradients in a form that's pairs of gradients and variables. That can then be applied back directly. This also gives us the opportunity to update any of the gradients, such as clipping them if they are too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum((Y - Y_pred) ** 2)\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=0.002)\n",
    "grads_and_vars = opt.compute_gradients(loss, [beta, b])\n",
    "\n",
    "# Could introduce clipping here.\n",
    "\n",
    "# This now does all the assignments at once.\n",
    "train_op = opt.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.0793245, 1.1530027)\n",
      "(3.2712493, 1.6980028)\n",
      "(3.962677, 1.9408524)\n",
      "(4.3689451, 2.0377469)\n",
      "(4.6108928, 2.0672009)\n",
      "(4.7569761, 2.0677917)\n",
      "(4.8463922, 2.0578327)\n",
      "(4.9018512, 2.0455444)\n",
      "(4.9366808, 2.034256)\n",
      "(4.9588065, 2.0250239)\n",
      "(4.9730077, 2.0179226)\n",
      "(4.9822054, 2.0126593)\n",
      "(4.9882092, 2.0088518)\n",
      "(4.9921546, 2.0061436)\n",
      "(4.9947619, 2.00424)\n",
      "(4.9964929, 2.0029137)\n",
      "(4.9976468, 2.0019958)\n",
      "(4.9984179, 2.0013635)\n",
      "(4.9989347, 2.0009298)\n",
      "(4.9992819, 2.000633)\n"
     ]
    }
   ],
   "source": [
    "for _ in xrange(20):\n",
    "    sess.run(train_op, feed_dict={x: X_train, Y: Y_train})\n",
    "    beta_np, b_np = sess.run([beta, b], feed_dict={x: X_train, Y: Y_train})\n",
    "    print(beta_np[0, 0], b_np[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple minimization\n",
    "If we don't need clipping, there's a simpler method we can use that handles all the work for us: `minimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum((Y - Y_pred) ** 2)\n",
    "# This automatically does symbolic differentiation for each\n",
    "# Variable in the graph, rather than having to specify\n",
    "# them all individually.\n",
    "train_op = (\n",
    "    tf.train.GradientDescentOptimizer(learning_rate=0.002)\n",
    "        .minimize(loss)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.0793245, 1.1530027)\n",
      "(3.2712493, 1.6980028)\n",
      "(3.962677, 1.9408524)\n",
      "(4.3689451, 2.0377469)\n",
      "(4.6108928, 2.0672009)\n",
      "(4.7569761, 2.0677917)\n",
      "(4.8463922, 2.0578327)\n",
      "(4.9018512, 2.0455444)\n",
      "(4.9366808, 2.034256)\n",
      "(4.9588065, 2.0250239)\n",
      "(4.9730077, 2.0179226)\n",
      "(4.9822054, 2.0126593)\n",
      "(4.9882092, 2.0088518)\n",
      "(4.9921546, 2.0061436)\n",
      "(4.9947619, 2.00424)\n",
      "(4.9964929, 2.0029137)\n",
      "(4.9976468, 2.0019958)\n",
      "(4.9984179, 2.0013635)\n",
      "(4.9989347, 2.0009298)\n",
      "(4.9992819, 2.000633)\n"
     ]
    }
   ],
   "source": [
    "for _ in xrange(20):\n",
    "    sess.run(train_op, feed_dict={x: X_train, Y: Y_train})\n",
    "    beta_np, b_np = sess.run([beta, b], feed_dict={x: X_train, Y: Y_train})\n",
    "    print(beta_np[0, 0], b_np[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simpler loss\n",
    "At this point, the only thing in the graph that's not part of the model or something \"magical\" like the Optimizer is the loss function. Wouldn't it be great if we didn't have to compute that by hand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(labels=Y, predictions=Y_pred)\n",
    "train_op = (\n",
    "    tf.train.GradientDescentOptimizer(learning_rate=0.2)\n",
    "        .minimize(loss)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.0793254, 1.1530026)\n",
      "(3.2712498, 1.6980028)\n",
      "(3.9626775, 1.9408524)\n",
      "(4.3689451, 2.0377469)\n",
      "(4.6108928, 2.0672009)\n",
      "(4.7569761, 2.0677917)\n",
      "(4.8463922, 2.0578327)\n",
      "(4.9018512, 2.0455444)\n",
      "(4.9366808, 2.034256)\n",
      "(4.9588065, 2.0250239)\n",
      "(4.9730077, 2.0179226)\n",
      "(4.9822054, 2.0126593)\n",
      "(4.9882092, 2.0088518)\n",
      "(4.9921546, 2.0061436)\n",
      "(4.9947619, 2.00424)\n",
      "(4.9964929, 2.002914)\n",
      "(4.9976468, 2.001996)\n",
      "(4.9984179, 2.0013638)\n",
      "(4.9989347, 2.0009298)\n",
      "(4.9992819, 2.000633)\n"
     ]
    }
   ],
   "source": [
    "for _ in xrange(20):\n",
    "    sess.run(train_op, feed_dict={x: X_train, Y: Y_train})\n",
    "    beta_np, b_np = sess.run([beta, b], feed_dict={x: X_train, Y: Y_train})\n",
    "    print(beta_np[0, 0], b_np[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customization\n",
    "Now that we have all the quick-and-easy built-in functions in play, we can start using some more advanced techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.39096761, 0.20299864)\n",
      "(0.79054809, 0.41175276)\n",
      "(1.1983778, 0.62564063)\n",
      "(1.614144, 0.84344792)\n",
      "(2.0375748, 1.0631342)\n",
      "(2.4682589, 1.2814939)\n",
      "(2.905431, 1.4897974)\n",
      "(3.3476224, 1.6850142)\n",
      "(3.7917595, 1.8693861)\n",
      "(4.2271633, 2.0307839)\n",
      "(4.607008, 2.1110363)\n",
      "(4.8318753, 2.0752444)\n",
      "(4.9300275, 2.0337627)\n",
      "(4.9727731, 2.0132186)\n",
      "(4.990294, 2.0047102)\n",
      "(4.9968781, 2.0015152)\n",
      "(4.9991102, 2.0004318)\n",
      "(4.9997807, 2.0001063)\n",
      "(4.9999547, 2.0000219)\n",
      "(4.9999928, 2.0000036)\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.huber_loss(labels=Y, predictions=Y_pred)\n",
    "train_op = (\n",
    "    tf.train.RMSPropOptimizer(learning_rate=0.5)\n",
    "        .minimize(loss)\n",
    ")\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for _ in xrange(20):\n",
    "    sess.run(train_op, feed_dict={x: X_train, Y: Y_train})\n",
    "    beta_np, b_np = sess.run([beta, b], feed_dict={x: X_train, Y: Y_train})\n",
    "    print(beta_np[0, 0], b_np[0, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
