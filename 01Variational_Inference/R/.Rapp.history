K = 5#
p_true = rgamma(K, 1, 1)#
p_true = p_true/sum(p_true
p_true = p_true/sum(p_true)
p_true
K = 5#
p_true = rgamma(K, 2, 1)#
p_true = p_true/sum(p_true)#
#
p_true
tau = 3
p_true = rgamma(K, 2, 1)#
p_true = p_true/sum(p_true)#
#
mu_true = rnorm(K, 0, 3)
N = 1000
# Fitting a discrete Gaussian mixture model by variational Bayes#
#
K = 5#
tau = 3#
#
N = 1000#
#
p_true = rgamma(K, 2, 1)#
p_true = p_true/sum(p_true)#
mu_true = rnorm(K, 0, 3)#
c_true = sample(K, size=N, replace=TRUE, prob=p_true)
c_true
# Fitting a discrete Gaussian mixture model by variational Bayes#
#
K = 5#
tau = 3#
#
N = 1000#
#
p_true = rgamma(K, 2, 1)#
p_true = p_true/sum(p_true)#
mu_true = rnorm(K, 0, 3)#
c_true = sample(K, size=N, replace=TRUE, prob=p_true)#
y = rnorm(N, mu_true[c_true], 1)
hist(y, 20)
phi_hat = matrix(1/K, nrow=N, ncol=K)
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)
psi_hat = outer(y, m_hat)
psi_hat
m_k^2 + s2_k
(m_hat^2 + s2_hat)
?repmat
?rep
psi_hat = outer(y, m_hat) - matrix(rep(m_hat^2 + s2_hat, N), ncol=N)
matrix(rep(m_hat^2 + s2_hat, N), ncol=N)
psi_hat = outer(y, m_hat) - t(matrix(rep(m_hat^2 + s2_hat, N), ncol=N))
psi_hat
epsi_hat = exp(psi_hat
epsi_hat = exp(psi_hat)
phi_hat = epsi_hat/rowSums(epsi_hat)
phi_hat
rowSums(phi_hat)
phi_hat
s2_hat = 1/{1/tau^2 + colSums(phi_hat)}
s2_hat
colSums(phi_hat)
crossprod(phi_hat, y)
m_hat = s2_hat * drop(crossprod(phi_hat, y))
m_hat
mu_true
for(i in 1:100) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - t(matrix(rep(m_hat^2 + s2_hat, N), ncol=N))#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/tau^2 + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
}
m_hat
m_true
mu_true
phi_hat
colMeans(phi_hat)
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
for(i in 1:100) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - t(matrix(rep(m_hat^2 + s2_hat/2, N), ncol=N))#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/tau^2 + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
}#
#
m_hat#
mu_true
for(i in 1:100) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - t(matrix(rep(m_hat^2 + s2_hat/2, N), ncol=N))#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/tau^2 + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
}#
#
m_hat
s2_hat
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
for(i in 1:100) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - t(matrix(rep(m_hat^2 + s2_hat/2, N), ncol=N))#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/tau^2 + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
}#
#
m_hat
for(i in 1:100) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - t(matrix(rep(m_hat^2 + s2_hat/2, N), ncol=N))#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/tau^2 + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
}#
#
m_hat
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
for(i in 1:100) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*t(matrix(rep(m_hat^2 + s2_hat, N), ncol=N))#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/tau^2 + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
}#
#
m_hat
for(i in 1:100) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*t(matrix(rep(m_hat^2 + s2_hat, N), ncol=N))#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/tau^2 + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
}#
#
m_hat
mu_true
# Fitting a discrete Gaussian mixture model by variational Bayes#
#
K = 5#
tau = 3#
#
N = 1000#
#
p_true = 1/K#
mu_true = rnorm(K, 0, 3)#
c_true = sample(K, size=N, replace=TRUE, prob=p_true)#
y = rnorm(N, mu_true[c_true], 1)#
#
hist(y, 20)
# Fitting a discrete Gaussian mixture model by variational Bayes#
#
K = 5#
tau = 3#
#
N = 1000#
#
p_true = rep(1/K, K)#
mu_true = rnorm(K, 0, 3)#
c_true = sample(K, size=N, replace=TRUE, prob=p_true)#
y = rnorm(N, mu_true[c_true], 1)#
#
hist(y, 20)
mu_true
K = 5#
tau = 3#
#
N = 1000#
#
p_true = rep(1/K, K)#
mu_true = rnorm(K, 0, 3)#
c_true = sample(K, size=N, replace=TRUE, prob=p_true)#
y = rnorm(N, mu_true[c_true], 1)#
#
hist(y, 50)
mu_true
library(FDRreg)
library(devtools)
install_github('jgscott/FDRreg', subdir="R_pkg/")
devtools::install_github('jgscott/FDRreg', subdir="R_pkg/")
K = 5#
tau = 3#
#
N = 1000#
#
p_true = rep(1/K, K)#
mu_true = rnorm(K, 0, 3)#
c_true = sample(K, size=N, replace=TRUE, prob=p_true)#
y = rnorm(N, mu_true[c_true], 1)#
#
hist(y, 50)#
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
for(i in 1:100) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*t(matrix(rep(m_hat^2 + s2_hat, N), ncol=N))#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/tau^2 + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
}#
#
m_hat#
mu_true
sort(m_hat)#
sort(mu_true)
hist(y, 50)
dnormix = function(x, mu) {#
	K = length(mu)#
	out = rep(0, length(x))#
	for(j in 1:K) {#
		out = out + (1/K)*dnorm(x, mu, 1)#
	}#
	out#
}
hist(y, 50, prob=TRUE)
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true), col='blue', lwd=2, add=TRUE)
mu_true
curve(dnormix(x, mu_true), from=-5, to=5)
dnormix = function(x, mu) {#
	K = length(mu)#
	out = rep(0, length(x))#
	for(j in 1:K) {#
		out = out + (1/K)*dnorm(x, mu[j], 1)#
	}#
	out#
}#
#
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true), col='blue', lwd=2, add=TRUE)
curve(dnormix(x, m_hat), col='red', lwd=2, add=TRUE)
for(i in 1:100) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*t(matrix(rep(m_hat^2 + s2_hat, N), ncol=N))#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/tau^2 + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
}#
#
sort(m_hat)#
sort(mu_true)
for(i in 1:100) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*t(matrix(rep(m_hat^2 + s2_hat, N), ncol=N))#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/tau^2 + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
}#
#
sort(m_hat)#
sort(mu_true)
for(i in 1:100) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*t(matrix(rep(m_hat^2 + s2_hat, N), ncol=N))#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/tau^2 + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
}#
#
sort(m_hat)#
sort(mu_true)
for(i in 1:100) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*t(matrix(rep(m_hat^2 + s2_hat, N), ncol=N))#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/tau^2 + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
}#
#
sort(m_hat)#
sort(mu_true)
for(i in 1:100) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*t(matrix(rep(m_hat^2 + s2_hat, N), ncol=N))#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/tau^2 + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
}#
#
sort(m_hat)#
sort(mu_true)
dnormix = function(x, mu) {#
	K = length(mu)#
	out = rep(0, length(x))#
	for(j in 1:K) {#
		out = out + (1/K)*dnorm(x, mu[j], 1)#
	}#
	out#
}#
#
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat), col='red', lwd=2, add=TRUE)
phi_hat
colMeans(phi_hat)
K = 5#
tau = 3#
#
N = 1000#
#
p_true = rep(1/K, K)#
mu_true = rnorm(K, 0, 3)#
c_true = sample(K, size=N, replace=TRUE, prob=p_true)#
y = rnorm(N, mu_true[c_true], 1)#
#
hist(y, 50)#
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
for(i in 1:100) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*t(matrix(rep(m_hat^2 + s2_hat, N), ncol=N))#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/tau^2 + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = #
}#
#
sort(m_hat)#
sort(mu_true)
for(i in 1:100) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*t(matrix(rep(m_hat^2 + s2_hat, N), ncol=N))#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/tau^2 + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = #
}#
#
sort(m_hat)#
sort(mu_true)
for(i in 1:100) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*t(matrix(rep(m_hat^2 + s2_hat, N), ncol=N))#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/tau^2 + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = #
}#
#
sort(m_hat)#
sort(mu_true)
for(i in 1:100) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*t(matrix(rep(m_hat^2 + s2_hat, N), ncol=N))#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/tau^2 + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = #
}#
#
sort(m_hat)#
sort(mu_true)
dnormix = function(x, mu) {#
	K = length(mu)#
	out = rep(0, length(x))#
	for(j in 1:K) {#
		out = out + (1/K)*dnorm(x, mu[j], 1)#
	}#
	out#
}#
#
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat), col='red', lwd=2, add=TRUE)
K = 5#
tau = 3#
#
N = 1000#
#
p_true = rep(1/K, K)#
mu_true = rnorm(K, 0, 3)#
c_true = sample(K, size=N, replace=TRUE, prob=p_true)
y = rnorm(N, mu_true[c_true], 1)
hist(y, 50)
hist(y, 50)#
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
for(i in 1:100) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*t(matrix(rep(m_hat^2 + s2_hat, N), ncol=N))#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/tau^2 + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = #
}#
#
sort(m_hat)#
sort(mu_true)
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat), col='red', lwd=2, add=TRUE)
outer(y, m_hat)
m_hat
t(matrix(rep(m_hat^2 + s2_hat, N), ncol=N)
t(matrix(rep(m_hat^2 + s2_hat, N), ncol=N))
t(matrix(rep(m_hat^2 + s2_hat, N), ncol=N))
m_hat^2 + s2_hat
rep_rows = function(x, n) {#
	# constructs a matrix by repeating a vector x over n rows#
	t(matrix(rep(x, n), ncol=n)#
}
rep_rows = function(x, n) {#
	# constructs a matrix by repeating a vector x over n rows#
	t(matrix(rep(x, n), ncol=n)#
}
rep_rows = function(x, n) {#
	# constructs a matrix by repeating a vector x over n rows#
	t(matrix(rep(x, n), ncol=n))#
}
rep_rows(c(1,2,3), 5)
psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N))
psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)
rep_rows(s2_hat + m_hat^2,10)
s2_hat + m_hat^2
psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)
psi_hat
epsi_hat = exp(psi_hat)
epsi_hat
phi_hat = epsi_hat/rowSums(epsi_hat)
phi_hat
y[1]
phi_hat[1,]
m_hat
s2_hat + m_hat^2
s2_hat
m_hat
y[1]
phi[1,]
phi_hat[1,]
colSums(phi_hat)
s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}
s2_hat
phi_hat
crossprod(phi_hat, y)
dim(phi_hat)
K = 5#
tau = 3#
#
N = 1000#
#
p_true = rep(1/K, K)#
mu_true = rnorm(K, 0, 3)#
c_true = sample(K, size=N, replace=TRUE, prob=p_true)#
y = rnorm(N, mu_true[c_true], 1)#
#
hist(y, 50)#
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
for(i in 1:100) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
}#
#
sort(m_hat)#
sort(mu_true)
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true), col='blue', lwd=2, add=TRUE)
curve(dnormix(x, m_hat), col='red', lwd=2, add=TRUE)
colMeans(phi_hat)
sort(m_hat)
sort(mu_true)
w_hat = colMeans(phi_hat)
w_hat
# utility for equal-weight Gaussian mixture#
dnormix = function(x, mu, w) {#
	K = length(mu)#
	out = rep(0, length(x))#
	for(j in 1:K) {#
		out = out + w[j]*dnorm(x, mu[j], 1)#
	}#
	out#
}
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true, rep(1/K, K)), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, w), col='red', lwd=2, add=TRUE)
curve(dnormix(x, m_hat, w_hat), col='red', lwd=2, add=TRUE)
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true, rep(1/K, K)), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, w_hat), col='red', lwd=2, add=TRUE)
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true, rep(1/K, K)), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, w_hat), col='red', lwd=2, add=TRUE)
psi_hat
phi_hat*psi_hat
E3 = sum(phi_hat*psi_hat)
E3
E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)
E1
E3
E4 = sum(phi_hat*log(phi_hat))
E4
p_true = rep(1/K, K)#
mu_true = rnorm(K, 0, 3)#
c_true = sample(K, size=N, replace=TRUE, prob=p_true)#
y = rnorm(N, mu_true[c_true], 1)#
#
hist(y, 50)#
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
n_steps = 100#
ELBO_tracker = rep(NA, n_steps)#
#
for(i in 1:n_steps) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)#
	E4 = sum(phi_hat*log(phi_hat))#
	ELBO_tracker[i] = E1 + E3 - E4#
}#
#
plot(ELBO_tracker)
plot(tail(ELBO_tracker, -10))
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true, rep(1/K, K)), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, colMeans(phi_hat)), col='red', lwd=2, add=TRUE)
# utility for equal-weight Gaussian mixture#
dnormix = function(x, mu, w) {#
	K = length(mu)#
	out = rep(0, length(x))#
	for(j in 1:K) {#
		out = out + w[j]*dnorm(x, mu[j], 1)#
	}#
	out#
}#
#
rep_rows = function(x, n) {#
	# constructs a matrix by repeating a vector x over n rows#
	t(matrix(rep(x, n), ncol=n))#
}#
K = 5#
tau = 3#
#
N = 1000#
#
p_true = rep(1/K, K)#
mu_true = rnorm(K, 0, 3)#
c_true = sample(K, size=N, replace=TRUE, prob=p_true)#
y = rnorm(N, mu_true[c_true], 1)#
#
hist(y, 50)#
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
n_steps = 100#
ELBO_tracker = rep(NA, n_steps)#
#
for(i in 1:n_steps) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)#
	E4 = sum(phi_hat*log(phi_hat))#
	ELBO_tracker[i] = E1 + E3 - E4#
}#
#
plot(tail(ELBO_tracker, -10))#
#
sort(m_hat)#
sort(mu_true)#
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true, rep(1/K, K)), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, colMeans(phi_hat)), col='red', lwd=2, add=TRUE)
# Note: defining the predictive distribution is tricky.#
# I'll cheat by calculating an average of the mixture indicators#
# and use these as weights in a "posterior" mixture model#
w_hat = colMeans(phi_hat)#
#
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true, rep(1/K, K)), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, w_hat), col='red', lwd=2, add=TRUE)
plot(tail(ELBO_tracker, -10))
sort(m_hat)
sort(mu_true)
w_hat
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
n_steps = 100#
ELBO_tracker = rep(NA, n_steps)#
#
for(i in 1:n_steps) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)#
	E4 = sum(phi_hat*log(phi_hat))#
	ELBO_tracker[i] = E1 + E3 - E4#
}#
#
plot(tail(ELBO_tracker, -10))#
#
sort(m_hat)#
sort(mu_true)
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
n_steps = 100#
ELBO_tracker = rep(NA, n_steps)#
#
for(i in 1:n_steps) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)#
	E4 = sum(phi_hat*log(phi_hat))#
	ELBO_tracker[i] = E1 + E3 - E4#
}#
#
plot(tail(ELBO_tracker, -10))#
#
sort(m_hat)#
sort(mu_true)
K = 5#
tau = 3#
#
N = 1000#
#
p_true = rep(1/K, K)#
mu_true = rnorm(K, 0, 3)#
c_true = sample(K, size=N, replace=TRUE, prob=p_true)#
y = rnorm(N, mu_true[c_true], 1)#
#
hist(y, 50)#
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
n_steps = 100#
ELBO_tracker = rep(NA, n_steps)#
#
for(i in 1:n_steps) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)#
	E4 = sum(phi_hat*log(phi_hat))#
	ELBO_tracker[i] = E1 + E3 - E4#
}#
#
plot(tail(ELBO_tracker, -10))#
#
sort(m_hat)#
sort(mu_true)
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true, rep(1/K, K)), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, w_hat), col='red', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, rep(1/K, K)), col='green', lwd=2, add=TRUE)
w_hat = colMeans(phi_hat)#
#
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true, rep(1/K, K)), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, w_hat), col='red', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, rep(1/K, K)), col='green', lwd=2, add=TRUE)
K = 5#
tau = 3#
#
N = 1000#
#
p_true = rep(1/K, K)#
mu_true = rnorm(K, 0, 3)#
c_true = sample(K, size=N, replace=TRUE, prob=p_true)#
y = rnorm(N, mu_true[c_true], 1)#
#
hist(y, 50)#
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
n_steps = 100#
ELBO_tracker = rep(NA, n_steps)#
#
for(i in 1:n_steps) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)#
	E4 = sum(phi_hat*log(phi_hat))#
	ELBO_tracker[i] = E1 + E3 - E4#
}#
#
plot(tail(ELBO_tracker, -10))#
#
sort(m_hat)#
sort(mu_true)#
#
# Note: defining the predictive distribution is tricky.#
# I'll cheat by calculating an average of the mixture indicators#
# and use these as weights in a "posterior" mixture model#
w_hat = colMeans(phi_hat)#
#
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true, rep(1/K, K)), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, w_hat), col='red', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, rep(1/K, K)), col='green', lwd=2, add=TRUE)
plot(tail(ELBO_tracker, -10))
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
n_steps = 1000#
ELBO_tracker = rep(NA, n_steps)#
#
for(i in 1:n_steps) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)#
	E4 = sum(phi_hat*log(phi_hat))#
	ELBO_tracker[i] = E1 + E3 - E4#
}#
#
plot(tail(ELBO_tracker, -10))
K = 5#
tau = 3#
N = 1000#
#
# Mixture model parameters#
p_true = rep(1/K, K)#
mu_true = rnorm(K, 0, 3)#
c_true = sample(K, size=N, replace=TRUE, prob=p_true)#
y = rnorm(N, mu_true[c_true], 1)#
#
hist(y, 50)#
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
n_steps = 200#
ELBO_tracker = rep(NA, n_steps)#
#
for(i in 1:n_steps) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)#
	E4 = sum(phi_hat*log(phi_hat))#
	ELBO_tracker[i] = E1 + E3 - E4#
}#
#
plot(tail(ELBO_tracker, -10))
n_steps = 100#
ELBO_tracker = rep(NA, n_steps)#
#
for(i in 1:n_steps) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)#
	E4 = sum(phi_hat*log(phi_hat))#
	ELBO_tracker[i] = E1 + E3 - E4#
}#
#
plot(tail(ELBO_tracker, -10))#
#
sort(m_hat)#
sort(mu_true)
ELBO_tracker
# Mixture model parameters#
p_true = rep(1/K, K)#
mu_true = rnorm(K, 0, 3)#
c_true = sample(K, size=N, replace=TRUE, prob=p_true)#
y = rnorm(N, mu_true[c_true], 1)#
#
hist(y, 50)#
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
n_steps = 100#
ELBO_tracker = rep(NA, n_steps)#
#
for(i in 1:n_steps) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)#
	E4 = sum(phi_hat*log(phi_hat))#
	ELBO_tracker[i] = E1 + E3 - E4#
}#
#
plot(tail(ELBO_tracker, -10))
# Note: defining the predictive distribution is tricky.#
# I'll cheat by calculating an average of the mixture indicators#
# and use these as weights in a "posterior" mixture model#
w_hat = colMeans(phi_hat)#
#
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true, rep(1/K, K)), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, w_hat), col='red', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, rep(1/K, K)), col='green', lwd=2, add=TRUE)
old_ELBO = -1e99
old_ELBO
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
max_steps = 1000#
rel_tol = 1e-6#
ELBO_tracker = rep(NA, max_steps)#
#
converged = FALSE#
step_counter = 1#
old_ELBO = -1e99#
while({!converged} & {step_counter <= max_steps}) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO (up to constant terms)#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)#
	E4 = sum(phi_hat*log(phi_hat))#
	new_ELBO = E1 + E3 - E4#
	ELBO_tracker[step_counter] = E1 + E3 - E4#
	# check convergence#
	delta = abs(new_ELBO - old_ELBO)#
	converged = delta/(abs(old_ELBO) + rel_tol) < rel_tol#
	if(!converged) {#
		old_ELBO = new_ELBO#
		step_counter = step_counter + 1#
	}#
}#
ELBO_tracker = na.omit(ELBO_tracker)
step_counter
plot(tail(ELBO_tracker, -10))
sort(m_hat)
sort(mu_true)
# Note: defining the predictive distribution is tricky.#
# I'll cheat by calculating an average of the mixture indicators#
# and use these as weights in a "posterior" mixture model#
w_hat = colMeans(phi_hat)#
#
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true, rep(1/K, K)), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, w_hat), col='red', lwd=2, add=TRUE)
# the naive unweighted predictive density#
# won't work well if we get "collapsing" of mixture components#
# in the variational approximation#
curve(dnormix(x, m_hat, rep(1/K, K)), col='green', lwd=2, add=TRUE)
converged
converged; step_counter
# hyperpars and sample size#
K = 5#
tau = 3#
N = 1000#
#
# Mixture model parameters#
p_true = rep(1/K, K)#
mu_true = rnorm(K, 0, 3)#
c_true = sample(K, size=N, replace=TRUE, prob=p_true)#
y = rnorm(N, mu_true[c_true], 1)#
#
hist(y, 50)#
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
max_steps = 1000#
rel_tol = 1e-6#
ELBO_tracker = rep(NA, max_steps)#
#
converged = FALSE#
step_counter = 1#
old_ELBO = -1e99#
while({!converged} & {step_counter <= max_steps}) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO (up to constant terms)#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)#
	E4 = sum(phi_hat*log(phi_hat))#
	new_ELBO = E1 + E3 - E4#
	ELBO_tracker[step_counter] = E1 + E3 - E4#
	# check convergence#
	delta = abs(new_ELBO - old_ELBO)#
	converged = delta/(abs(old_ELBO) + rel_tol) < rel_tol#
	if(!converged) {#
		old_ELBO = new_ELBO#
		step_counter = step_counter + 1#
	}#
}#
ELBO_tracker = na.omit(ELBO_tracker)#
converged; step_counter#
#
plot(tail(ELBO_tracker, -5))
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
max_steps = 1000#
rel_tol = 1e-6#
ELBO_tracker = rep(NA, max_steps)
converged = FALSE#
step_counter = 1#
old_ELBO = -1e99#
while({!converged} & {step_counter <= max_steps}) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO (up to constant terms)#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)#
	E4 = sum(phi_hat*log(phi_hat))#
	new_ELBO = E1 + E3 - E4#
	ELBO_tracker[step_counter] = E1 + E3 - E4#
	# check convergence#
	delta = abs(new_ELBO - old_ELBO)#
	converged = delta/(abs(old_ELBO) + rel_tol) < rel_tol#
	if(!converged) {#
		old_ELBO = new_ELBO#
		step_counter = step_counter + 1#
	}#
}
ELBO_tracker = na.omit(ELBO_tracker)
converged; step_counter
plot(tail(ELBO_tracker, -5))
sort(m_hat)#
sort(mu_true)
-1e1000
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
max_steps = 1000#
rel_tol = 1e-6#
ELBO_tracker = rep(NA, max_steps)#
#
converged = FALSE#
step_counter = 1#
old_ELBO = -1e100#
while({!converged} & {step_counter <= max_steps}) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO (up to constant terms)#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)#
	E4 = sum(phi_hat*log(phi_hat))#
	new_ELBO = E1 + E3 - E4#
	ELBO_tracker[step_counter] = E1 + E3 - E4#
	# check convergence#
	delta = abs(new_ELBO - old_ELBO)#
	converged = delta/(abs(old_ELBO) + rel_tol) < rel_tol#
	if(!converged) {#
		old_ELBO = new_ELBO#
		step_counter = step_counter + 1#
	}#
}#
ELBO_tracker = na.omit(ELBO_tracker)#
converged; step_counter#
#
plot(tail(ELBO_tracker, -5))
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
max_steps = 1000#
rel_tol = 1e-9#
ELBO_tracker = rep(NA, max_steps)#
#
converged = FALSE#
step_counter = 1#
old_ELBO = -1e100#
while({!converged} & {step_counter <= max_steps}) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO (up to constant terms)#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)#
	E4 = sum(phi_hat*log(phi_hat))#
	new_ELBO = E1 + E3 - E4#
	ELBO_tracker[step_counter] = E1 + E3 - E4#
	# check convergence#
	delta = abs(new_ELBO - old_ELBO)#
	converged = delta/(abs(old_ELBO) + rel_tol) < rel_tol#
	if(!converged) {#
		old_ELBO = new_ELBO#
		step_counter = step_counter + 1#
	}#
}#
ELBO_tracker = na.omit(ELBO_tracker)#
converged; step_counter
plot(tail(ELBO_tracker, -5))
sort(m_hat)#
sort(mu_true)
# Note: defining the predictive distribution is tricky.#
# I'll cheat by calculating an average of the mixture indicators#
# and use these as weights in a "posterior" mixture model#
w_hat = colMeans(phi_hat)#
#
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true, rep(1/K, K)), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, w_hat), col='red', lwd=2, add=TRUE)#
#
# the naive unweighted predictive density#
# won't work well if we get "collapsing" of mixture components#
# in the variational approximation#
curve(dnormix(x, m_hat, rep(1/K, K)), col='green', lwd=2, add=TRUE)
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true, rep(1/K, K)), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, w_hat), col='red', lwd=2, add=TRUE)#
#
# the naive unweighted predictive density#
# won't work well if we get "collapsing" of mixture components#
# in the variational approximation#
curve(dnormix(x, m_hat, rep(1/K, K)), col='green', lwd=2, add=TRUE)
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true, rep(1/K, K)), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, w_hat), col='red', lwd=2, add=TRUE)#
#
# the naive unweighted predictive density#
# won't work well if we get "collapsing" of mixture components#
# in the variational approximation#
curve(dnormix(x, m_hat, rep(1/K, K)), col='green', lwd=2, add=TRUE)#
legend('topleft', lwd=2, col=c('blue', 'red', 'green'),#
	legend=c("True", "Ad-hoc Weighted", "Naive Unweighted")_)
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true, rep(1/K, K)), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, w_hat), col='red', lwd=2, add=TRUE)#
#
# the naive unweighted predictive density#
# won't work well if we get "collapsing" of mixture components#
# in the variational approximation#
curve(dnormix(x, m_hat, rep(1/K, K)), col='green', lwd=2, add=TRUE)#
legend('topleft', lwd=2, col=c('blue', 'red', 'green'),#
	legend=c("True", "Ad-hoc Weighted", "Naive Unweighted"))
# Fitting a discrete Gaussian mixture model by variational Bayes#
# Coordinate-ascent variational inference#
#
# utility for equal-variance Gaussian mixture#
dnormix = function(x, mu, w) {#
	K = length(mu)#
	out = rep(0, length(x))#
	for(j in 1:K) {#
		out = out + w[j]*dnorm(x, mu[j], 1)#
	}#
	out#
}
# constructs a matrix by repeating a vector x over n rows#
# like repmat in Matlab#
rep_rows = function(x, n) {#
	t(matrix(rep(x, n), ncol=n))#
}
# hyperpars and sample size#
K = 5#
tau = 3#
N = 1000
p_true = rep(1/K, K)
mu_true = rnorm(K, 0, 3)
mu_true
c_true = sample(K, size=N, replace=TRUE, prob=p_true)
c_true
y = rnorm(N, mu_true[c_true], 1)
hist(y, 50)
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)
max_steps = 1000
rel_tol = 1e-9
ELBO_tracker = rep(NA, max_steps)
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
max_steps = 1000#
rel_tol = 1e-9#
ELBO_tracker = rep(NA, max_steps)#
#
converged = FALSE#
step_counter = 1#
old_ELBO = -1e100#
while({!converged} & {step_counter <= max_steps}) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO (up to constant terms)#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)#
	E4 = sum(phi_hat*log(phi_hat))#
	new_ELBO = E1 + E3 - E4#
	ELBO_tracker[step_counter] = E1 + E3 - E4#
	# check convergence#
	delta = abs(new_ELBO - old_ELBO)#
	converged = delta/(abs(old_ELBO) + rel_tol) < rel_tol#
	if(!converged) {#
		old_ELBO = new_ELBO#
		step_counter = step_counter + 1#
	}#
}#
ELBO_tracker = na.omit(ELBO_tracker)#
converged; step_counter
plot(tail(ELBO_tracker, -5))
plot(tail(ELBO_tracker, -10))
sort(m_hat)
sort(mu_true)
# Note: defining the predictive distribution is tricky.#
# I'll cheat by calculating an average of the mixture indicators#
# and use these as weights in a "posterior" mixture model#
w_hat = colMeans(phi_hat)#
#
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true, rep(1/K, K)), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, w_hat), col='red', lwd=2, add=TRUE)#
#
# the naive unweighted predictive density#
# won't work well if we get "collapsing" of mixture components#
# in the variational approximation#
curve(dnormix(x, m_hat, rep(1/K, K)), col='green', lwd=2, add=TRUE)#
legend('topleft', lwd=2, col=c('blue', 'red', 'green'),#
	legend=c("True", "Ad-hoc Weighted", "Naive Unweighted"))
# constructs a matrix by repeating a vector x over n rows#
# like repmat in Matlab#
rep_rows = function(x, n) {#
	t(matrix(rep(x, n), ncol=n))#
}#
#
# hyperpars and sample size#
K = 5#
tau = 3#
N = 1000#
#
# Mixture model parameters#
p_true = rep(1/K, K)#
mu_true = rnorm(K, 0, 3)#
c_true = sample(K, size=N, replace=TRUE, prob=p_true)#
y = rnorm(N, mu_true[c_true], 1)#
#
hist(y, 50)#
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
max_steps = 1000#
rel_tol = 1e-9#
ELBO_tracker = rep(NA, max_steps)#
#
converged = FALSE#
step_counter = 1#
old_ELBO = -1e100#
while({!converged} & {step_counter <= max_steps}) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO (up to constant terms)#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)#
	E4 = sum(phi_hat*log(phi_hat))#
	new_ELBO = E1 + E3 - E4#
	ELBO_tracker[step_counter] = E1 + E3 - E4#
	# check convergence#
	delta = abs(new_ELBO - old_ELBO)#
	converged = delta/(abs(old_ELBO) + rel_tol) < rel_tol#
	if(!converged) {#
		old_ELBO = new_ELBO#
		step_counter = step_counter + 1#
	}#
}#
ELBO_tracker = na.omit(ELBO_tracker)#
converged; step_counter#
#
plot(tail(ELBO_tracker, -10))#
#
sort(m_hat)#
sort(mu_true)#
#
# Note: defining the predictive distribution is tricky.#
# I'll cheat by calculating an average of the mixture indicators#
# and use these as weights in a "posterior" mixture model#
w_hat = colMeans(phi_hat)#
#
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true, rep(1/K, K)), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, w_hat), col='red', lwd=2, add=TRUE)#
#
# the naive unweighted predictive density#
# won't work well if we get "collapsing" of mixture components#
# in the variational approximation#
curve(dnormix(x, m_hat, rep(1/K, K)), col='green', lwd=2, add=TRUE)#
legend('topleft', lwd=2, col=c('blue', 'red', 'green'),#
	legend=c("True", "Ad-hoc Weighted", "Naive Unweighted"))
# constructs a matrix by repeating a vector x over n rows#
# like repmat in Matlab#
rep_rows = function(x, n) {#
	t(matrix(rep(x, n), ncol=n))#
}#
#
# hyperpars and sample size#
K = 5#
tau = 3#
N = 1000#
#
# Mixture model parameters#
p_true = rep(1/K, K)#
mu_true = rnorm(K, 0, 3)#
c_true = sample(K, size=N, replace=TRUE, prob=p_true)#
y = rnorm(N, mu_true[c_true], 1)#
#
hist(y, 50)#
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
max_steps = 1000#
rel_tol = 1e-9#
ELBO_tracker = rep(NA, max_steps)#
#
converged = FALSE#
step_counter = 1#
old_ELBO = -1e100#
while({!converged} & {step_counter <= max_steps}) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO (up to constant terms)#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)#
	E4 = sum(phi_hat*log(phi_hat))#
	new_ELBO = E1 + E3 - E4#
	ELBO_tracker[step_counter] = E1 + E3 - E4#
	# check convergence#
	delta = abs(new_ELBO - old_ELBO)#
	converged = delta/(abs(old_ELBO) + rel_tol) < rel_tol#
	if(!converged) {#
		old_ELBO = new_ELBO#
		step_counter = step_counter + 1#
	}#
}#
ELBO_tracker = na.omit(ELBO_tracker)#
converged; step_counter#
#
plot(tail(ELBO_tracker, -10))#
#
sort(m_hat)#
sort(mu_true)#
#
# Note: defining the predictive distribution is tricky.#
# I'll cheat by calculating an average of the mixture indicators#
# and use these as weights in a "posterior" mixture model#
w_hat = colMeans(phi_hat)#
#
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true, rep(1/K, K)), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, w_hat), col='red', lwd=2, add=TRUE)#
#
# the naive unweighted predictive density#
# won't work well if we get "collapsing" of mixture components#
# in the variational approximation#
curve(dnormix(x, m_hat, rep(1/K, K)), col='green', lwd=2, add=TRUE)#
legend('topleft', lwd=2, col=c('blue', 'red', 'green'),#
	legend=c("True", "Ad-hoc Weighted", "Naive Unweighted"))
# constructs a matrix by repeating a vector x over n rows#
# like repmat in Matlab#
rep_rows = function(x, n) {#
	t(matrix(rep(x, n), ncol=n))#
}#
#
# hyperpars and sample size#
K = 5#
tau = 3#
N = 1000#
#
# Mixture model parameters#
p_true = rep(1/K, K)#
mu_true = rnorm(K, 0, 3)#
c_true = sample(K, size=N, replace=TRUE, prob=p_true)#
y = rnorm(N, mu_true[c_true], 1)#
#
hist(y, 50)#
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
max_steps = 1000#
rel_tol = 1e-9#
ELBO_tracker = rep(NA, max_steps)#
#
converged = FALSE#
step_counter = 1#
old_ELBO = -1e100#
while({!converged} & {step_counter <= max_steps}) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO (up to constant terms)#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)#
	E4 = sum(phi_hat*log(phi_hat))#
	new_ELBO = E1 + E3 - E4#
	ELBO_tracker[step_counter] = E1 + E3 - E4#
	# check convergence#
	delta = abs(new_ELBO - old_ELBO)#
	converged = delta/(abs(old_ELBO) + rel_tol) < rel_tol#
	if(!converged) {#
		old_ELBO = new_ELBO#
		step_counter = step_counter + 1#
	}#
}#
ELBO_tracker = na.omit(ELBO_tracker)#
converged; step_counter#
#
plot(tail(ELBO_tracker, -10))#
#
sort(m_hat)#
sort(mu_true)#
#
# Note: defining the predictive distribution is tricky.#
# I'll cheat by calculating an average of the mixture indicators#
# and use these as weights in a "posterior" mixture model#
w_hat = colMeans(phi_hat)#
#
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true, rep(1/K, K)), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, w_hat), col='red', lwd=2, add=TRUE)#
#
# the naive unweighted predictive density#
# won't work well if we get "collapsing" of mixture components#
# in the variational approximation#
curve(dnormix(x, m_hat, rep(1/K, K)), col='green', lwd=2, add=TRUE)#
legend('topleft', lwd=2, col=c('blue', 'red', 'green'),#
	legend=c("True", "Ad-hoc Weighted", "Naive Unweighted"))
# constructs a matrix by repeating a vector x over n rows#
# like repmat in Matlab#
rep_rows = function(x, n) {#
	t(matrix(rep(x, n), ncol=n))#
}#
#
# hyperpars and sample size#
K = 5#
tau = 3#
N = 1000#
#
# Mixture model parameters#
p_true = rep(1/K, K)#
mu_true = rnorm(K, 0, 3)#
c_true = sample(K, size=N, replace=TRUE, prob=p_true)#
y = rnorm(N, mu_true[c_true], 1)#
#
hist(y, 50)#
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
max_steps = 1000#
rel_tol = 1e-9#
ELBO_tracker = rep(NA, max_steps)#
#
converged = FALSE#
step_counter = 1#
old_ELBO = -1e100#
while({!converged} & {step_counter <= max_steps}) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO (up to constant terms)#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)#
	E4 = sum(phi_hat*log(phi_hat))#
	new_ELBO = E1 + E3 - E4#
	ELBO_tracker[step_counter] = E1 + E3 - E4#
	# check convergence#
	delta = abs(new_ELBO - old_ELBO)#
	converged = delta/(abs(old_ELBO) + rel_tol) < rel_tol#
	if(!converged) {#
		old_ELBO = new_ELBO#
		step_counter = step_counter + 1#
	}#
}#
ELBO_tracker = na.omit(ELBO_tracker)#
converged; step_counter#
#
plot(tail(ELBO_tracker, -10))#
#
sort(m_hat)#
sort(mu_true)#
#
# Note: defining the predictive distribution is tricky.#
# I'll cheat by calculating an average of the mixture indicators#
# and use these as weights in a "posterior" mixture model#
w_hat = colMeans(phi_hat)#
#
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true, rep(1/K, K)), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, w_hat), col='red', lwd=2, add=TRUE)#
#
# the naive unweighted predictive density#
# won't work well if we get "collapsing" of mixture components#
# in the variational approximation#
curve(dnormix(x, m_hat, rep(1/K, K)), col='green', lwd=2, add=TRUE)#
legend('topleft', lwd=2, col=c('blue', 'red', 'green'),#
	legend=c("True", "Ad-hoc Weighted", "Naive Unweighted"))
# constructs a matrix by repeating a vector x over n rows#
# like repmat in Matlab#
rep_rows = function(x, n) {#
	t(matrix(rep(x, n), ncol=n))#
}#
#
# hyperpars and sample size#
K = 5#
tau = 3#
N = 1000#
#
# Mixture model parameters#
p_true = rep(1/K, K)#
mu_true = rnorm(K, 0, 3)#
c_true = sample(K, size=N, replace=TRUE, prob=p_true)#
y = rnorm(N, mu_true[c_true], 1)#
#
hist(y, 50)#
# initialize variational parameters#
m_hat = rnorm(K, 0, 1)#
s2_hat = rgamma(K, 2, 2)#
#
max_steps = 1000#
rel_tol = 1e-9#
ELBO_tracker = rep(NA, max_steps)#
#
converged = FALSE#
step_counter = 1#
old_ELBO = -1e100#
while({!converged} & {step_counter <= max_steps}) {#
	# update variational distribution for cluster indicators#
	psi_hat = outer(y, m_hat) - 0.5*rep_rows(s2_hat + m_hat^2, N)#
	epsi_hat = exp(psi_hat)#
	phi_hat = epsi_hat/rowSums(epsi_hat)#
	# Update variational distribution for mixture components#
	s2_hat = 1/{1/(tau^2) + colSums(phi_hat)}#
	m_hat = s2_hat * drop(crossprod(phi_hat, y))#
	# Check ELBO (up to constant terms)#
	E1 = {-1/(2*tau^2)}*sum(s2_hat + m_hat^2)#
	E3 = sum(phi_hat*psi_hat)#
	E4 = sum(phi_hat*log(phi_hat))#
	new_ELBO = E1 + E3 - E4#
	ELBO_tracker[step_counter] = E1 + E3 - E4#
	# check convergence#
	delta = abs(new_ELBO - old_ELBO)#
	converged = delta/(abs(old_ELBO) + rel_tol) < rel_tol#
	if(!converged) {#
		old_ELBO = new_ELBO#
		step_counter = step_counter + 1#
	}#
}#
ELBO_tracker = na.omit(ELBO_tracker)#
converged; step_counter#
#
plot(tail(ELBO_tracker, -10))#
#
sort(m_hat)#
sort(mu_true)#
#
# Note: defining the predictive distribution is tricky.#
# I'll cheat by calculating an average of the mixture indicators#
# and use these as weights in a "posterior" mixture model#
w_hat = colMeans(phi_hat)#
#
hist(y, 50, prob=TRUE)#
curve(dnormix(x, mu_true, rep(1/K, K)), col='blue', lwd=2, add=TRUE)#
curve(dnormix(x, m_hat, w_hat), col='red', lwd=2, add=TRUE)#
#
# the naive unweighted predictive density#
# won't work well if we get "collapsing" of mixture components#
# in the variational approximation#
curve(dnormix(x, m_hat, rep(1/K, K)), col='green', lwd=2, add=TRUE)#
legend('topleft', lwd=2, col=c('blue', 'red', 'green'),#
	legend=c("True", "Ad-hoc Weighted", "Naive Unweighted"))
